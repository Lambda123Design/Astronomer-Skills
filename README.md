# Astronomer and Airflow -Skills
This repository details my skills on Astronomer, for running Airflow

**A) Setting up Airflow with Astro**

**B) Building Your First DAG With Airflow**

**C) Designing Mathematical Calculation DAG With Airflow**

**D) Getting Started With TaskFlow API Using Apache Airflow**


**A) Setting up Airflow with Astro**

1. To Initialize Project - "astro dev init".

2. To start Airflow - "astro dev start"

3. To stop Airflow - "astro dev stop"

ðŸ”¹ Components

Astronomer (Astro): A managed platform for Apache Airflow that simplifies running, scaling, monitoring, and securing Airflow.

Docker: Required since Astronomer runs Airflow within Docker containers.

Docker Compose: Used to enable multiple services (e.g., Airflow webserver, scheduler, database) to interact.

Project Structure (auto-created by astro dev init):

.astro â†’ Astro project config.

dags/ â†’ Folder for DAGs.

Dockerfile â†’ Image definition for running Airflow.

include/ â†’ For additional files.

plugins/ â†’ For Airflow plugins.

tests/ â†’ For test cases.

requirements.txt â†’ For extra Python dependencies.

README.md â†’ Project info.

Airflow UI (localhost:8080): Web interface for managing DAGs.

XCom: Mechanism for passing information between tasks.

ðŸ”¹ Important Astronomer Commands

Initialize project:

astro dev init


â†’ Creates the Astro project structure with DAGs, Dockerfile, configs.

Start Airflow:

astro dev start


â†’ Builds Docker image, runs Airflow containers (webserver, scheduler, Postgres, triggerer).

Stop Airflow:

astro dev stop


â†’ Stops all running Airflow containers.

ðŸ”¹ Example DAG (default)

example_dag.py provided in dags/.

Demonstrates an ETL workflow:

Task 1 â€“ Get Astronauts â†’ Fetch astronauts from API.

Task 2 â€“ Print Astronauts â†’ Print list of astronauts.

ðŸ”¹ Steps to Set Up and Run Airflow with Astro

Install Docker (prerequisite).

Create project folder (e.g., airflow-astro).

Open terminal / VS Code and navigate to folder.

Initialize project â†’ astro dev init.

Inspect structure â†’ dags/, Dockerfile, .astro, etc.

Start Airflow â†’ astro dev start.

Containers for webserver, scheduler, Postgres created.

Airflow UI accessible at http://localhost:8080.

Default credentials: admin / admin.

Check DAGs in UI â†’ example_astronauts.

View tasks (get_astronauts, print_astronauts).

View Graph, Gantt, Logs, Code, XCom, etc.

Trigger DAG manually or let it follow its schedule.

Monitor logs & XComs for task details.

Stop Airflow â†’ astro dev stop.



**B) Building Your First DAG With Airflow**

Components Used

DAG (Directed Acyclic Graph) â†’ Defines the workflow.

PythonOperator â†’ Runs Python functions as tasks.

datetime â†’ Used for scheduling (start date).

Tasks Defined:

preprocess_data() â†’ Pre-processing step.

train_model() â†’ Training step.

evaluate_model() â†’ Evaluation step.

ðŸ”¹ DAG Definition
with DAG(
    dag_id="ml_pipeline",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@weekly",
    catchup=False
) as dag:


dag_id = "ml_pipeline"

start_date = 2024-01-01

schedule_interval = "@weekly" (runs once a week).

catchup=False â†’ Prevents backfilling past runs.

ðŸ”¹ Tasks Setup (PythonOperator)
preprocess = PythonOperator(
    task_id="preprocess_task",
    python_callable=preprocess_data
)

train = PythonOperator(
    task_id="train_task",
    python_callable=train_model
)

evaluate = PythonOperator(
    task_id="evaluate_task",
    python_callable=evaluate_model
)


Each task has a unique task_id.

python_callable points to the defined Python function.

ðŸ”¹ Dependencies
preprocess >> train >> evaluate


Execution order: Preprocessing â†’ Training â†’ Evaluation.

If one task fails, downstream tasks wonâ€™t run.

ðŸ”¹ Full DAG File â†’ dags/ml_pipeline.py

Contains imports, task functions, DAG definition, operators, and dependencies.

Saved in dags/ folder so Airflow can auto-detect it.

ðŸ”¹ Running the Pipeline

Start Airflow:

astro dev start


â†’ Brings up Docker containers (Airflow webserver, scheduler, Postgres).

Access UI:

Open: http://localhost:8080

Login: admin / admin (default).

View DAGs:

example_dag (default).

ml_pipeline (new DAG).

Inside ml_pipeline DAG â†’ see tasks:

preprocess_task â†’ Logs: â€œPre-processing data...â€

train_task â†’ Logs: â€œTraining model...â€

evaluate_task â†’ Logs: â€œEvaluating model...â€

Graph view shows arrows â†’ Preprocess â†’ Train â†’ Evaluate.

Successful runs show green tasks in UI.

ðŸ”¹ Managing Airflow

Stop containers:

astro dev stop


Restart (reload code changes):

astro dev restart

**C) Designing Mathematical Calculation DAG With Airflow**

Key Concept: XCom (Cross Communication)

Mechanism for tasks to share data in Airflow.

Uses:

xcom_push(key, value) â†’ Push value into XCom.

xcom_pull(key, task_ids) â†’ Retrieve value from another task.

Enables data flow between tasks.

ðŸ”¹ DAG File â†’ dags/maths_operation.py
Imports
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

ðŸ”¹ Task Functions

Start with 10

def start_number(**context):
    value = 10
    context['ti'].xcom_push(key='current_value', value=value)
    print(f"Starting number is {value}")


Add 5

def add_five(**context):
    current_value = context['ti'].xcom_pull(key='current_value', task_ids='start_task')
    new_value = current_value + 5
    context['ti'].xcom_push(key='current_value', value=new_value)
    print(f"{current_value} + 5 = {new_value}")


Multiply by 2

def multiply_by_two(**context):
    current_value = context['ti'].xcom_pull(key='current_value', task_ids='add_five_task')
    new_value = current_value * 2
    context['ti'].xcom_push(key='current_value', value=new_value)
    print(f"{current_value} * 2 = {new_value}")


Subtract 3

def subtract_three(**context):
    current_value = context['ti'].xcom_pull(key='current_value', task_ids='multiply_by_two_task')
    new_value = current_value - 3
    context['ti'].xcom_push(key='current_value', value=new_value)
    print(f"{current_value} - 3 = {new_value}")


Square the number

def square_number(**context):
    current_value = context['ti'].xcom_pull(key='current_value', task_ids='subtract_three_task')
    new_value = current_value ** 2
    print(f"{current_value}^2 = {new_value}")

ðŸ”¹ DAG Definition
with DAG(
    dag_id="maths_sequence_dag",
    start_date=datetime(2023, 1, 1),
    schedule_interval="@once",
    catchup=False
) as dag:


dag_id = "maths_sequence_dag"

start_date = 2023-01-01

schedule_interval = "@once" â†’ Runs only when manually triggered.

catchup=False â†’ No backfilling.

ðŸ”¹ Tasks Setup (with XCom context)
start_task = PythonOperator(
    task_id="start_task",
    python_callable=start_number,
    provide_context=True
)
# Similar for add_five_task, multiply_by_two_task, subtract_three_task, square_task


Each task has provide_context=True â†’ makes context available for XCom push/pull.

ðŸ”¹ Dependencies
start_task >> add_five_task >> multiply_by_two_task >> subtract_three_task >> square_task


Execution order:
Start (10) â†’ Add Five (15) â†’ Multiply (30) â†’ Subtract (27) â†’ Square (729)

ðŸ”¹ Running the DAG

Start Airflow:

astro dev start


Open UI: http://localhost:8080 â†’ login admin/admin.

See new DAG â†’ maths_sequence_dag.

Trigger DAG manually.

ðŸ”¹ Monitoring Results

Graph view â†’ tasks connected in sequence.

Logs:

Start: Starting number is 10

Add: 10 + 5 = 15

Multiply: 15 * 2 = 30

Subtract: 30 - 3 = 27

Square: 27^2 = 729

XCom tab â†’ shows values passed: 10 â†’ 15 â†’ 30 â†’ 27.

This example introduces XComs for passing data between tasks.

Next step: TaskFlow API (with @task decorator) â†’ makes this simpler without manually handling xcom_push / xcom_pull.

**D) Getting Started With TaskFlow API Using Apache Airflow**

Key Concept: TaskFlow API (introduced in Airflow 2.0)

Lets you define tasks with @task decorator.

Return values from tasks are automatically stored in XCom.

No need to write xcom_push / xcom_pull.

Task dependencies look like normal Python function calls.

Makes DAGs cleaner, simpler, and more Pythonic.

ðŸ”¹ DAG File â†’ dags/taskflow_api.py
Imports
from airflow import DAG
from airflow.decorators import task
from datetime import datetime

ðŸ”¹ DAG Definition
with DAG(
    dag_id="maths_sequence_taskflow",
    start_date=datetime(2023, 1, 1),
    schedule_interval="@once",
    catchup=False
) as dag:


dag_id = "maths_sequence_taskflow"

schedule_interval = "@once" â†’ run only when triggered manually.

catchup=False â†’ no backfilling.

ðŸ”¹ Task Functions with @task

Start with 10

@task
def start_number():
    initial_value = 10
    print(f"Starting number is {initial_value}")
    return initial_value


Add 5

@task
def add_five(number):
    new_value = number + 5
    print(f"{number} + 5 = {new_value}")
    return new_value


Multiply by 2

@task
def multiply_by_two(number):
    new_value = number * 2
    print(f"{number} * 2 = {new_value}")
    return new_value


Subtract 3

@task
def subtract_three(number):
    new_value = number - 3
    print(f"{number} - 3 = {new_value}")
    return new_value


Square

@task
def square_number(number):
    new_value = number ** 2
    print(f"{number}^2 = {new_value}")
    return new_value

ðŸ”¹ Task Dependencies (looks like normal Python calls)
start_val = start_number()
added_val = add_five(start_val)
multiplied_val = multiply_by_two(added_val)
subtracted_val = subtract_three(multiplied_val)
squared_val = square_number(subtracted_val)


Behind the scenes â†’ Airflow builds DAG structure.

Dependencies:
start â†’ add_five â†’ multiply â†’ subtract â†’ square

ðŸ”¹ Running the DAG

Start Airflow:

astro dev start


Open Airflow UI â†’ http://localhost:8080

Trigger maths_sequence_taskflow DAG.

ðŸ”¹ Monitoring Results

Graph View â†’ tasks connected in sequence.

Logs:

start_number: Starting number is 10

add_five: 10 + 5 = 15

multiply_by_two: 15 * 2 = 30

subtract_three: 30 - 3 = 27

square_number: 27^2 = 729

XCom tab: return values (10 â†’ 15 â†’ 30 â†’ 27 â†’ 729) are auto-stored.

**Conclusion**

Compared to PythonOperator + manual XComs, TaskFlow API:

Is cleaner and requires less boilerplate.

Automatically handles XComs & dependencies.

Feels like writing normal Python code, but produces a DAG.

Next, we can dive into advanced TaskFlow features (like retries, retries with exponential backoff, custom XCom backends, branching).


Is cleaner and requires less boilerplate.

Automatically handles XComs & dependencies.

Feels like writing normal Python code, but produces a DAG.

Next, we can dive into advanced TaskFlow features (like retries, retries with exponential backoff, custom XCom backends, branching).
